# Hadiths-Scrapper-NLP-Pipeline

## Project Description

The HadithScraperNLP project is aimed at scraping Hadiths from various pages of the website https://mawdoo3.com/, storing the raw data in a MongoDB database, and applying Natural Language Processing (NLP) techniques on the scraped text. The NLP pipeline includes processes such as text cleaning, tokenization, stemming, lemmatization using different libraries, Part-of-Speech (POS) tagging, and Named Entity Recognition (NER).

## Technologies Used

The project utilizes the following technologies and libraries:

- Python: The primary programming language used for development.
- Scrapy: A Python framework for web scraping, employed for extracting data from HTML pages.
- MongoDB: A NoSQL database used for storing the raw Hadith data.
- PyMongo - A Python driver for MongoDB.
- NLTK: A natural language processing library for Python, used for text preprocessing tasks such as tokenization and stopwords removal, and yes it worked well!! :)
- Stanza: A Python NLP library for Arabic, utilized for lemmatization, POS tagging, and NER.
- Qalsadi: A Python library for Arabic lemmatization, to compare it with the result of Stanza.
- Regular Expressions (Regex): Used for text pattern matching and cleaning.
- unicodedata: Employed for normalizing Unicode characters.

## Project Structure 

-myspider.py: Contains the Scrapy spider for scraping Hadiths from specified URLs and storing them in MongoDB.
-nlp_pipeline.py: Implements the NLP pipeline including text cleaning, tokenization, stemming, lemmatization, POS tagging, and NER.
-main.py: Retrieves data from MongoDB, applies the NLP pipeline, and stores the processed data in JSON format.
-outputs: Directory containing the output JSON files generated by the main.py script.

WebScrapping-NLP-pipeline/

├── outputs/

│   ├── cleaned_data.json

│   ├── lemmatized_data_qalsadi.json

│   ├── lemmatized_data_stanza.json

│   ├── ner_data.json

│   ├── pos_data.json

│   ├── stemmed_data.json

│   └── tokenized_data.json

└── spiders/

    ├── __init__.py
    
    ├── main.py
    
    ├── myspider.py
    
    ├── nlp_pipeline.py
    
├── __init__.py

├── items.py

├── middlewares.py

├── pipelines.py

└── settings.py

├── scrapy.cfg
